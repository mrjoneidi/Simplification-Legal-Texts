{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu\n",
    "!pip install torch_optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import sys  \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_scheduler, Seq2SeqTrainingArguments, Seq2SeqTrainer, GenerationConfig\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_optimizer import Lamb\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from huggingface_hub import HfApi, HfFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-02T22:55:06.055537Z",
     "iopub.status.busy": "2025-04-02T22:55:06.055201Z",
     "iopub.status.idle": "2025-04-02T22:55:07.137925Z",
     "shell.execute_reply": "2025-04-02T22:55:07.136974Z",
     "shell.execute_reply.started": "2025-04-02T22:55:06.055486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset from the JSON file\n",
    "with open(\"/kaggle/input/divan-ali/Divan_ali_Simplified.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:55:39.517121Z",
     "iopub.status.busy": "2025-04-02T22:55:39.516359Z",
     "iopub.status.idle": "2025-04-02T22:55:39.527358Z",
     "shell.execute_reply": "2025-04-02T22:55:39.526565Z",
     "shell.execute_reply.started": "2025-04-02T22:55:39.517089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store original and simplified texts\n",
    "texts = []\n",
    "simplified_texts = []\n",
    "\n",
    "# Extract original and simplified texts\n",
    "for entry in data:\n",
    "    for key, content in entry.items():\n",
    "        original_text = content.get(\"متن رای\", \"\")\n",
    "        simplified_text = content.get(\"simplified text\", \"\")\n",
    "\n",
    "        # Ensure both original and simplified texts are not empty\n",
    "        if original_text and simplified_text:\n",
    "            texts.append(original_text)\n",
    "            simplified_texts.append(simplified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:55:53.726744Z",
     "iopub.status.busy": "2025-04-02T22:55:53.726063Z",
     "iopub.status.idle": "2025-04-02T22:55:53.731237Z",
     "shell.execute_reply": "2025-04-02T22:55:53.730383Z",
     "shell.execute_reply.started": "2025-04-02T22:55:53.726691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/kaggle/input/nunlimiformer/') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:56:47.135395Z",
     "iopub.status.busy": "2025-04-02T22:56:47.135057Z",
     "iopub.status.idle": "2025-04-02T22:57:12.807886Z",
     "shell.execute_reply": "2025-04-02T22:57:12.807015Z",
     "shell.execute_reply.started": "2025-04-02T22:56:47.135366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315f40ecc1d240e0887d241897d4af58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce30a6c2abb41a59a7ba0ecc5aedc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fae3d2975ee407eb2de19ecfbd6ad1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567b9b1393584d4f9f66a7f6662dce3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17ae1bb6d2b460e84616c9e08582006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fdf81e9fa4483db39c877abd1e63c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unlimiformer import UnlimiformerT5\n",
    "\n",
    "\n",
    "model_name_or_path = \"Moryjj/pt5_la9\"\n",
    "use_auth_token = False  # Set to False if no authentication is needed\n",
    "max_target_length = 512  # Maximum length of the target sequence\n",
    "fp16 = True  # Enable mixed precision training\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    use_auth_token=use_auth_token\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    use_auth_token=use_auth_token\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Unlimiformer for ParsT5\n",
    "unlimiformer = UnlimiformerT5(\n",
    "    model=model, \n",
    "    layer_begin=10,  # Layer to start from for Unlimiformer indexing\n",
    "    layer_end=11,  # Ending layer (or None to include all following layers)\n",
    "    unlimiformer_head_num=None,\n",
    "    exclude_attention=False,\n",
    "    model_encoder_max_len=4096,\n",
    "    chunk_overlap=0.5,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:57:12.810669Z",
     "iopub.status.busy": "2025-04-02T22:57:12.809845Z",
     "iopub.status.idle": "2025-04-02T22:57:12.815684Z",
     "shell.execute_reply": "2025-04-02T22:57:12.814715Z",
     "shell.execute_reply.started": "2025-04-02T22:57:12.810624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from index_building import DatastoreBatch\n",
    "\n",
    "\n",
    "# Initialize the datastore\n",
    "datastore = DatastoreBatch(\n",
    "    dim=unlimiformer.model.config.d_model,  # Dimension of model embeddings\n",
    "    batch_size=8,  # Adjust batch size based on memory availability\n",
    "    flat_index=False,  # Use inverted file index for efficient retrieval\n",
    "    gpu_index=True,  # Enable GPU indexing\n",
    "    index_device=torch.device(\"cuda\")  # GPU device for index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:57:12.817147Z",
     "iopub.status.busy": "2025-04-02T22:57:12.816922Z",
     "iopub.status.idle": "2025-04-02T22:57:12.873438Z",
     "shell.execute_reply": "2025-04-02T22:57:12.872726Z",
     "shell.execute_reply.started": "2025-04-02T22:57:12.817125Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input_ids shape: torch.Size([4096])\n",
      "Sample attention_mask shape: torch.Size([4096])\n",
      "Sample labels shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "inputs = texts\n",
    "targets = simplified_texts\n",
    "\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, max_chunk_length=1024, max_sequence_length=4096, chunk_overlap=512):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.max_chunk_length = max_chunk_length\n",
    "        self.max_sequence_length = max_sequence_length  # Set a fixed max length for input_ids\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        tokenized_input = tokenizer(input_text, return_tensors=\"pt\", truncation=False)\n",
    "\n",
    "        # Create overlapping chunks and concatenate them up to max_sequence_length\n",
    "        input_ids_chunks = []\n",
    "        for i in range(0, tokenized_input['input_ids'].shape[1], self.max_chunk_length - self.chunk_overlap):\n",
    "            chunk = tokenized_input['input_ids'][:, i:i + self.max_chunk_length]\n",
    "            if chunk.shape[1] < self.max_chunk_length:\n",
    "                chunk = torch.cat([chunk, torch.full((1, self.max_chunk_length - chunk.shape[1]), tokenizer.pad_token_id)], dim=1)\n",
    "            input_ids_chunks.append(chunk)\n",
    "        \n",
    "        # Concatenate chunks and truncate to max_sequence_length\n",
    "        full_input_ids = torch.cat(input_ids_chunks, dim=1)[:, :self.max_sequence_length]\n",
    "        attention_mask = (full_input_ids != tokenizer.pad_token_id).long()\n",
    "        \n",
    "        # Pad/truncate the input to max_sequence_length\n",
    "        if full_input_ids.shape[1] < self.max_sequence_length:\n",
    "            padding_length = self.max_sequence_length - full_input_ids.shape[1]\n",
    "            full_input_ids = torch.cat([full_input_ids, torch.full((1, padding_length), tokenizer.pad_token_id)], dim=1)\n",
    "            attention_mask = torch.cat([attention_mask, torch.zeros((1, padding_length), dtype=torch.long)], dim=1)\n",
    "        \n",
    "        # Tokenize target text and pad/truncate to max length of 512\n",
    "        target_text = self.targets[idx]\n",
    "        tokenized_target = tokenizer(target_text, return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": full_input_ids.squeeze(0),\n",
    "            \"attention_mask\": attention_mask.squeeze(0),\n",
    "            \"labels\": tokenized_target[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "# Verify the structure of a sample item\n",
    "sample = SummarizationDataset(inputs, targets).__getitem__(0)\n",
    "print(\"Sample input_ids shape:\", sample[\"input_ids\"].shape)\n",
    "print(\"Sample attention_mask shape:\", sample[\"attention_mask\"].shape)\n",
    "print(\"Sample labels shape:\", sample[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:57:15.437869Z",
     "iopub.status.busy": "2025-04-02T22:57:15.437388Z",
     "iopub.status.idle": "2025-04-02T22:57:15.946562Z",
     "shell.execute_reply": "2025-04-02T22:57:15.945701Z",
     "shell.execute_reply.started": "2025-04-02T22:57:15.437838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4522\n",
      "Validation size: 199\n",
      "Test size: 599\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "inputs = texts\n",
    "targets = simplified_texts # List of target summaries\n",
    "\n",
    "# First, split 80% for training and 20% for the remaining set (which will be split further)\n",
    "train_inputs, remaining_inputs, train_targets, remaining_targets = train_test_split(\n",
    "    inputs, targets, train_size =0.85, test_size = 0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Then, split the remaining 20% into 15% for test and 5% for validation\n",
    "val_inputs, test_inputs, val_targets, test_targets = train_test_split(\n",
    "    remaining_inputs, remaining_targets, test_size=0.75, random_state=42\n",
    ")\n",
    "\n",
    "# Verify split sizes\n",
    "print(\"Train size:\", len(train_inputs))\n",
    "print(\"Validation size:\", len(val_inputs))\n",
    "print(\"Test size:\", len(test_inputs))\n",
    "\n",
    "# Define dataset objects for each split\n",
    "train_dataset = SummarizationDataset(train_inputs, train_targets)\n",
    "val_dataset = SummarizationDataset(val_inputs, val_targets)\n",
    "test_dataset = SummarizationDataset(test_inputs, test_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:57:18.032195Z",
     "iopub.status.busy": "2025-04-02T22:57:18.031642Z",
     "iopub.status.idle": "2025-04-02T22:57:18.046763Z",
     "shell.execute_reply": "2025-04-02T22:57:18.045719Z",
     "shell.execute_reply.started": "2025-04-02T22:57:18.032163Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before freezing layers:\n",
      "Total parameters: 247539456\n",
      "Trainable parameters: 247539456\n",
      "Non-trainable parameters: 0\n",
      "Unfreezing layer: shared.weight\n",
      "Unfreezing layer: encoder.block.11.layer.0.SelfAttention.q.weight\n",
      "Unfreezing layer: encoder.block.11.layer.0.SelfAttention.k.weight\n",
      "Unfreezing layer: encoder.block.11.layer.0.SelfAttention.v.weight\n",
      "Unfreezing layer: encoder.block.11.layer.0.SelfAttention.o.weight\n",
      "Unfreezing layer: encoder.block.11.layer.0.layer_norm.weight\n",
      "Unfreezing layer: encoder.block.11.layer.1.DenseReluDense.wi_0.weight\n",
      "Unfreezing layer: encoder.block.11.layer.1.DenseReluDense.wi_1.weight\n",
      "Unfreezing layer: encoder.block.11.layer.1.DenseReluDense.wo.weight\n",
      "Unfreezing layer: encoder.block.11.layer.1.layer_norm.weight\n",
      "Unfreezing layer: decoder.block.11.layer.0.SelfAttention.q.weight\n",
      "Unfreezing layer: decoder.block.11.layer.0.SelfAttention.k.weight\n",
      "Unfreezing layer: decoder.block.11.layer.0.SelfAttention.v.weight\n",
      "Unfreezing layer: decoder.block.11.layer.0.SelfAttention.o.weight\n",
      "Unfreezing layer: decoder.block.11.layer.0.layer_norm.weight\n",
      "Unfreezing layer: decoder.block.11.layer.1.EncDecAttention.q.weight\n",
      "Unfreezing layer: decoder.block.11.layer.1.EncDecAttention.k.weight\n",
      "Unfreezing layer: decoder.block.11.layer.1.EncDecAttention.v.weight\n",
      "Unfreezing layer: decoder.block.11.layer.1.EncDecAttention.o.weight\n",
      "Unfreezing layer: decoder.block.11.layer.1.layer_norm.weight\n",
      "Unfreezing layer: decoder.block.11.layer.2.DenseReluDense.wi_0.weight\n",
      "Unfreezing layer: decoder.block.11.layer.2.DenseReluDense.wi_1.weight\n",
      "Unfreezing layer: decoder.block.11.layer.2.DenseReluDense.wo.weight\n",
      "Unfreezing layer: decoder.block.11.layer.2.layer_norm.weight\n",
      "Unfreezing layer: lm_head.weight\n",
      "After freezing layers:\n",
      "Total parameters: 247539456\n",
      "Trainable parameters: 65829120\n",
      "Non-trainable parameters: 181710336\n"
     ]
    }
   ],
   "source": [
    "# Function to print the number of total, trainable, and non-trainable parameters\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "# Print parameter counts before freezing layers\n",
    "print(\"Before freezing layers:\")\n",
    "count_parameters(unlimiformer.model)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last three layers by adjusting the names\n",
    "for name, param in model.named_parameters():\n",
    "    if 'encoder.block.11' in name or 'decoder.block.11' in name or 'lm_head' in name or 'shared' in name :\n",
    "            param.requires_grad = True\n",
    "            print(f\"Unfreezing layer: {name}\")\n",
    "\n",
    "# Print the number of parameters after freezing\n",
    "print(\"After freezing layers:\")\n",
    "count_parameters(unlimiformer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:57:32.852646Z",
     "iopub.status.busy": "2025-04-02T22:57:32.851989Z",
     "iopub.status.idle": "2025-04-02T22:57:33.466986Z",
     "shell.execute_reply": "2025-04-02T22:57:33.466092Z",
     "shell.execute_reply.started": "2025-04-02T22:57:32.852601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = Lamb(\n",
    "    filter(lambda p: p.requires_grad, unlimiformer.model.parameters()),\n",
    "    lr=4e-5,\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=400,\n",
    "    num_training_steps=4522\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:57:33.468469Z",
     "iopub.status.busy": "2025-04-02T22:57:33.468091Z",
     "iopub.status.idle": "2025-04-02T22:57:33.531934Z",
     "shell.execute_reply": "2025-04-02T22:57:33.531025Z",
     "shell.execute_reply.started": "2025-04-02T22:57:33.468442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory to save checkpoints\n",
    "    report_to=\"none\",  # Disable reporting for this setup\n",
    "    save_steps=200,  # Save model checkpoints less frequently\n",
    "    save_total_limit=1,  # Save only the last 3 checkpoints\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    do_train=True,  # Perform training\n",
    "    do_eval=True,  # Perform evaluation\n",
    "    per_device_train_batch_size=1,  # Increase batch size if memory allows\n",
    "    per_device_eval_batch_size=2,  # Increase batch size for evaluation\n",
    "    num_train_epochs=1,  # 10.27 to \n",
    "    seed=42,  # Ensure reproducibility\n",
    "    warmup_ratio=0.1,  # Keep warmup ratio for smoother start\n",
    "    weight_decay=0.01,  # Weight decay to avoid overfitting\n",
    "    learning_rate=3e-5,  # Reduced learning rate\n",
    "    logging_dir=\"./logs\",  # Directory for logs\n",
    "    logging_steps=200,# Log every 50 steps\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps = 200, # Evaluate during training\n",
    "    label_smoothing_factor=0.1,  # Apply label smoothing for better generalization\n",
    "    fp16=False,  # Enable mixed precision for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:57:45.521750Z",
     "iopub.status.busy": "2025-04-02T22:57:45.521045Z",
     "iopub.status.idle": "2025-04-02T22:57:45.527805Z",
     "shell.execute_reply": "2025-04-02T22:57:45.526824Z",
     "shell.execute_reply.started": "2025-04-02T22:57:45.521704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom Seq2SeqTrainer with overridden compute_loss method\n",
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        Compute custom loss for summarization/simplification.\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Ensure labels are on the correct device (same as model)\n",
    "        labels = labels.to(model.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Ensure that the logits and labels are correctly aligned\n",
    "        # logits: [batch_size, seq_len, vocab_size]\n",
    "        # labels: [batch_size, seq_len]\n",
    "        \n",
    "        # Flatten the logits and labels to compute the loss across the sequence\n",
    "        loss_fn = nn.CrossEntropyLoss(\n",
    "            ignore_index=self.tokenizer.pad_token_id,  # ignore padding token\n",
    "            reduction='mean'  # Optionally set reduction to 'mean' or 'sum'\n",
    "        )\n",
    "        \n",
    "        # Flatten logits and labels for loss calculation\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        \n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:57:45.529293Z",
     "iopub.status.busy": "2025-04-02T22:57:45.528934Z",
     "iopub.status.idle": "2025-04-02T22:57:46.043201Z",
     "shell.execute_reply": "2025-04-02T22:57:46.042463Z",
     "shell.execute_reply.started": "2025-04-02T22:57:45.529260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure the model has a default GenerationConfig\n",
    "if not hasattr(unlimiformer.model, \"generation_config\"):\n",
    "    unlimiformer.model.generation_config = GenerationConfig()\n",
    "\n",
    "# Instantiate the custom trainer\n",
    "trainer = CustomSeq2SeqTrainer(\n",
    "    model=unlimiformer.model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Pushing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:57:46.051022Z",
     "iopub.status.busy": "2025-04-02T22:57:46.050609Z",
     "iopub.status.idle": "2025-04-02T22:57:46.060931Z",
     "shell.execute_reply": "2025-04-02T22:57:46.060247Z",
     "shell.execute_reply.started": "2025-04-02T22:57:46.050984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save your Hugging Face token\n",
    "huggingface_token = \"YOURAPITOKEN\"  # Replace with your actual token\n",
    "HfFolder.save_token(huggingface_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:57:46.062951Z",
     "iopub.status.busy": "2025-04-02T22:57:46.062696Z",
     "iopub.status.idle": "2025-04-02T22:57:46.072552Z",
     "shell.execute_reply": "2025-04-02T22:57:46.071831Z",
     "shell.execute_reply.started": "2025-04-02T22:57:46.062927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "repo_name = \"pt5_la10\"  # Replace with your username and desired model name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "model = trainer.model #access the trained model from trainer.\n",
    "tokenizer = trainer.tokenizer #access the tokenizer from trainer.\n",
    "\n",
    "\n",
    "# Push the model and tokenizer to the Hugging Face Hub\n",
    "model.push_to_hub(repo_name, use_auth_token=huggingface_token)\n",
    "tokenizer.push_to_hub(repo_name, use_auth_token=huggingface_token)\n",
    "\n",
    "\n",
    "print(f\"Model and tokenizer pushed to Hugging Face Hub under {repo_name}!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5878069,
     "sourceId": 9629029,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6092684,
     "sourceId": 9914873,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
