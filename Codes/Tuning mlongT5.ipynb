{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:16:09.468536Z",
     "iopub.status.busy": "2025-03-11T16:16:09.468331Z",
     "iopub.status.idle": "2025-03-11T16:16:43.667828Z",
     "shell.execute_reply": "2025-03-11T16:16:43.666697Z",
     "shell.execute_reply.started": "2025-03-11T16:16:09.468516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.44.2\n",
      "Uninstalling transformers-4.44.2:\n",
      "  Successfully uninstalled transformers-4.44.2\n",
      "Found existing installation: torch 2.4.1+cu121\n",
      "Uninstalling torch-2.4.1+cu121:\n",
      "  Successfully uninstalled torch-2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:16:44.056164Z",
     "iopub.status.busy": "2025-03-11T16:16:44.055884Z",
     "iopub.status.idle": "2025-03-11T16:18:48.361843Z",
     "shell.execute_reply": "2025-03-11T16:18:48.360795Z",
     "shell.execute_reply.started": "2025-03-11T16:16:44.056143Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.45.1\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.4.5)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2024.8.30)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Successfully installed tokenizers-0.20.3 transformers-4.45.1\n",
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.45.1\n",
    "!pip install torch==2.4.0\n",
    "!pip install torch_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq, get_scheduler\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch_optimizer import Lamb\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import HfApi, HfFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:18:56.335081Z",
     "iopub.status.busy": "2025-03-11T16:18:56.334757Z",
     "iopub.status.idle": "2025-03-11T16:19:21.378028Z",
     "shell.execute_reply": "2025-03-11T16:19:21.377310Z",
     "shell.execute_reply.started": "2025-03-11T16:18:56.335054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d992101c2e804f598f82efb1516f08d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/906 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5561b7bdae984fedaadf9efc249bafe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819e9f2a605541ddb8777cf7d79b8664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13023401b0414e0589db9db744eb6d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60af15dc8e0433d8d54fd7dddd6c0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32328128ccd4462b3f5b71bba7c4e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"agemagician/mlong-t5-tglobal-base\"\n",
    "use_auth_token = False  # Set to False if no authentication is needed\n",
    "max_target_length = 512  # Maximum length of the target sequence\n",
    "fp16 = True  # Enable mixed precision training\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    use_auth_token=use_auth_token\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    use_auth_token=use_auth_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:19:24.900006Z",
     "iopub.status.busy": "2025-03-11T16:19:24.899572Z",
     "iopub.status.idle": "2025-03-11T16:19:26.656664Z",
     "shell.execute_reply": "2025-03-11T16:19:26.655717Z",
     "shell.execute_reply.started": "2025-03-11T16:19:24.899983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset from the JSON file\n",
    "with open(\"/kaggle/input/divan-ali/Divan_ali_Simplified.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# Initialize lists to store original and simplified texts\n",
    "texts = []\n",
    "simplified_texts = []\n",
    "\n",
    "# Extract original and simplified texts\n",
    "for entry in data:\n",
    "    for key, content in entry.items():\n",
    "        original_text = content.get(\"متن رای\", \"\")\n",
    "        simplified_text = content.get(\"simplified text\", \"\")\n",
    "\n",
    "        # Ensure both original and simplified texts are not empty\n",
    "        if original_text and simplified_text:\n",
    "            texts.append(original_text)\n",
    "            simplified_texts.append(simplified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:19:30.039548Z",
     "iopub.status.busy": "2025-03-11T16:19:30.039226Z",
     "iopub.status.idle": "2025-03-11T16:19:39.971115Z",
     "shell.execute_reply": "2025-03-11T16:19:39.970224Z",
     "shell.execute_reply.started": "2025-03-11T16:19:30.039521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "class LegalDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_input_length=4096, max_output_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)  # Load the list of dictionaries\n",
    "\n",
    "        # Flatten the nested structure into a list of relevant entries\n",
    "        self.data = []\n",
    "        for case in raw_data:\n",
    "            for case_id, details in case.items():\n",
    "                if \"متن رای\" in details and \"simplified text\" in details:\n",
    "                    self.data.append({\n",
    "                        \"متن رای\": details[\"متن رای\"],\n",
    "                        \"simplified text\": details[\"simplified text\"]\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_text = item[\"متن رای\"]\n",
    "        target_text = item[\"simplified text\"]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        targets = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_output_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Ensure correct type and handle padding tokens\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0).long()\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0).long()\n",
    "        labels = targets[\"input_ids\"].squeeze(0).long()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:19:39.972992Z",
     "iopub.status.busy": "2025-03-11T16:19:39.972334Z",
     "iopub.status.idle": "2025-03-11T16:19:40.454385Z",
     "shell.execute_reply": "2025-03-11T16:19:40.453678Z",
     "shell.execute_reply.started": "2025-03-11T16:19:39.972960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "data_file = \"/kaggle/input/divan-ali/Divan_ali_Simplified.json\"\n",
    "dataset = LegalDataset(data_file, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:20:14.073405Z",
     "iopub.status.busy": "2025-03-11T16:20:14.073056Z",
     "iopub.status.idle": "2025-03-11T16:20:14.077802Z",
     "shell.execute_reply": "2025-03-11T16:20:14.077001Z",
     "shell.execute_reply.started": "2025-03-11T16:20:14.073378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_size = 4522\n",
    "test_size = 599\n",
    "val_size = 199\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:20:21.888209Z",
     "iopub.status.busy": "2025-03-11T16:20:21.887764Z",
     "iopub.status.idle": "2025-03-11T16:20:21.914493Z",
     "shell.execute_reply": "2025-03-11T16:20:21.913571Z",
     "shell.execute_reply.started": "2025-03-11T16:20:21.888153Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before freezing layers:\n",
      "Total parameters: 985850496\n",
      "Trainable parameters: 985850496\n",
      "Non-trainable parameters: 0\n",
      "Unfreezing layer: shared.weight\n",
      "Unfreezing layer: encoder.block.11.layer.0.TransientGlobalSelfAttention.q.weight\n",
      "Unfreezing layer: encoder.block.11.layer.0.TransientGlobalSelfAttention.k.weight\n",
      "Unfreezing layer: encoder.block.11.layer.0.TransientGlobalSelfAttention.v.weight\n",
      "Unfreezing layer: encoder.block.11.layer.0.TransientGlobalSelfAttention.o.weight\n",
      "Unfreezing layer: encoder.block.11.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight\n",
      "Unfreezing layer: encoder.block.11.layer.0.layer_norm.weight\n",
      "Unfreezing layer: encoder.block.11.layer.1.DenseReluDense.wi_0.weight\n",
      "Unfreezing layer: encoder.block.11.layer.1.DenseReluDense.wi_1.weight\n",
      "Unfreezing layer: encoder.block.11.layer.1.DenseReluDense.wo.weight\n",
      "Unfreezing layer: encoder.block.11.layer.1.layer_norm.weight\n",
      "Unfreezing layer: decoder.block.11.layer.0.SelfAttention.q.weight\n",
      "Unfreezing layer: decoder.block.11.layer.0.SelfAttention.k.weight\n",
      "Unfreezing layer: decoder.block.11.layer.0.SelfAttention.v.weight\n",
      "Unfreezing layer: decoder.block.11.layer.0.SelfAttention.o.weight\n",
      "Unfreezing layer: decoder.block.11.layer.0.layer_norm.weight\n",
      "Unfreezing layer: decoder.block.11.layer.1.EncDecAttention.q.weight\n",
      "Unfreezing layer: decoder.block.11.layer.1.EncDecAttention.k.weight\n",
      "Unfreezing layer: decoder.block.11.layer.1.EncDecAttention.v.weight\n",
      "Unfreezing layer: decoder.block.11.layer.1.EncDecAttention.o.weight\n",
      "Unfreezing layer: decoder.block.11.layer.1.layer_norm.weight\n",
      "Unfreezing layer: decoder.block.11.layer.2.DenseReluDense.wi_0.weight\n",
      "Unfreezing layer: decoder.block.11.layer.2.DenseReluDense.wi_1.weight\n",
      "Unfreezing layer: decoder.block.11.layer.2.DenseReluDense.wo.weight\n",
      "Unfreezing layer: decoder.block.11.layer.2.layer_norm.weight\n",
      "Unfreezing layer: lm_head.weight\n",
      "After freezing layers:\n",
      "Total parameters: 985850496\n",
      "Trainable parameters: 410325504\n",
      "Non-trainable parameters: 575524992\n"
     ]
    }
   ],
   "source": [
    "# Function to print the number of total, trainable, and non-trainable parameters\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "# Print parameter counts before freezing layers\n",
    "print(\"Before freezing layers:\")\n",
    "count_parameters(model)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last three layers by adjusting the names\n",
    "for name, param in model.named_parameters():\n",
    "    if 'encoder.block.11' in name or 'decoder.block.11' in name or 'lm_head' in name or 'shared' in name :\n",
    "            param.requires_grad = True\n",
    "            print(f\"Unfreezing layer: {name}\")\n",
    "\n",
    "# Print the number of parameters after freezing\n",
    "print(\"After freezing layers:\")\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:20:25.113135Z",
     "iopub.status.busy": "2025-03-11T16:20:25.112852Z",
     "iopub.status.idle": "2025-03-11T16:20:25.218913Z",
     "shell.execute_reply": "2025-03-11T16:20:25.218254Z",
     "shell.execute_reply.started": "2025-03-11T16:20:25.113115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory to save checkpoints\n",
    "    report_to=\"none\",  # Disable reporting for this setup\n",
    "    save_steps=10,  # Save model checkpoints less frequently\n",
    "    save_total_limit=2,  # Save only the last 3 checkpoints\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    do_train=True,  # Perform training\n",
    "    do_eval=True,  # Perform evaluation\n",
    "    per_device_train_batch_size=1,  # Increase batch size if memory allows\n",
    "    per_device_eval_batch_size=2,  # Increase batch size for evaluation\n",
    "    num_train_epochs=3,  # Increase the number of epochs\n",
    "    seed=42,  # Ensure reproducibility\n",
    "    warmup_ratio=0.1,  # Keep warmup ratio for smoother start\n",
    "    weight_decay=0.01,  # Weight decay to avoid overfitting\n",
    "    learning_rate=3e-5,  # Reduced learning rate\n",
    "    logging_dir=\"./logs\",  # Directory for logs\n",
    "    logging_steps=10,  # Log every 50 steps\n",
    "    eval_strategy=\"steps\",  # Evaluate during training\n",
    "    eval_steps=10,  # Perform evaluation less frequently\n",
    "    label_smoothing_factor=0.1,  # Apply label smoothing for better generalization\n",
    "    fp16=False,  # Enable mixed precision for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:20:30.725328Z",
     "iopub.status.busy": "2025-03-11T16:20:30.725011Z",
     "iopub.status.idle": "2025-03-11T16:20:31.141059Z",
     "shell.execute_reply": "2025-03-11T16:20:31.140217Z",
     "shell.execute_reply.started": "2025-03-11T16:20:30.725306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = Lamb(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=5e-4,\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=170,\n",
    "    num_training_steps=1695\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:20:31.142911Z",
     "iopub.status.busy": "2025-03-11T16:20:31.142592Z",
     "iopub.status.idle": "2025-03-11T16:20:31.305123Z",
     "shell.execute_reply": "2025-03-11T16:20:31.303874Z",
     "shell.execute_reply.started": "2025-03-11T16:20:31.142881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:20:31.306972Z",
     "iopub.status.busy": "2025-03-11T16:20:31.306671Z",
     "iopub.status.idle": "2025-03-11T16:20:32.672040Z",
     "shell.execute_reply": "2025-03-11T16:20:32.670963Z",
     "shell.execute_reply.started": "2025-03-11T16:20:31.306947Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongT5ForConditionalGeneration(\n",
       "  (shared): Embedding(256384, 768)\n",
       "  (encoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(256384, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerTransientGlobalSelfAttention(\n",
       "            (TransientGlobalSelfAttention): LongT5TransientGlobalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "              (global_relative_attention_bias): Embedding(32, 12)\n",
       "              (global_input_layer_norm): LongT5LayerNorm()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerTransientGlobalSelfAttention(\n",
       "            (TransientGlobalSelfAttention): LongT5TransientGlobalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (global_input_layer_norm): LongT5LayerNorm()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(256384, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=256384, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:22:11.367926Z",
     "iopub.status.busy": "2025-03-11T16:22:11.367615Z",
     "iopub.status.idle": "2025-03-11T16:22:11.373498Z",
     "shell.execute_reply": "2025-03-11T16:22:11.372553Z",
     "shell.execute_reply.started": "2025-03-11T16:22:11.367903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqTrainerWithLoss(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Shift logits and labels for loss computation\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:22:12.770683Z",
     "iopub.status.busy": "2025-03-11T16:22:12.770359Z",
     "iopub.status.idle": "2025-03-11T16:22:12.800815Z",
     "shell.execute_reply": "2025-03-11T16:22:12.800152Z",
     "shell.execute_reply.started": "2025-03-11T16:22:12.770656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainerWithLoss(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, lr_scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Pushing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:22:17.086346Z",
     "iopub.status.busy": "2025-03-11T16:22:17.085984Z",
     "iopub.status.idle": "2025-03-11T16:22:17.091016Z",
     "shell.execute_reply": "2025-03-11T16:22:17.090138Z",
     "shell.execute_reply.started": "2025-03-11T16:22:17.086317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save your Hugging Face token\n",
    "huggingface_token = \"YOURTOKENAPI\"  # Replace with your actual token\n",
    "HfFolder.save_token(huggingface_token)\n",
    "\n",
    "repo_name = \"USERNAME/REPONAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Push the model and tokenizer to the Hugging Face Hub\n",
    "model.push_to_hub(repo_name, use_auth_token=huggingface_token)\n",
    "tokenizer.push_to_hub(repo_name, use_auth_token=huggingface_token)\n",
    "\n",
    "print(f\"Model and tokenizer pushed to Hugging Face Hub under {repo_name}!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5878069,
     "sourceId": 9629029,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5902265,
     "sourceId": 9660722,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6533780,
     "sourceId": 10560501,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
